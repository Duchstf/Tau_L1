{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 14:49:43.171675: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Import stuff\n",
    "import uproot4\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import curve_fit\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optparse\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "import hist\n",
    "from hist import Hist\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.ROOT)\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'medium',\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'medium',\n",
    "         'ytick.labelsize':'medium'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "#line thickness\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First make the rate plot for the old NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Should multiply the rate by **32MHz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = uproot4.open(\"../ntuples/Jan_17_2023/test_sig_v8.root\")\n",
    "bkg = uproot4.open(\"../ntuples/Jan_17_2023/test_bkg_v8.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ../models/L1Tau_v3_corrected.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m unique_sig_event \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39masarray(sig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mntuplePupSingle\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39marray()))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Load model and select out the tau\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models/L1Tau_v3_corrected.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Sig\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sig_input \u001b[38;5;241m=\u001b[39m sig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mntuplePupSingle\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtree\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39marray()\n",
      "File \u001b[0;32m/work/submit/dhoang/miniconda3/envs/tau_training/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/work/submit/dhoang/miniconda3/envs/tau_training/lib/python3.8/site-packages/keras/saving/save.py:226\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         )\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    232\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    233\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at ../models/L1Tau_v3_corrected.h5"
     ]
    }
   ],
   "source": [
    "unique_bkg_event = np.unique(np.asarray(bkg['ntuplePupSingle']['tree']['event'].array()))\n",
    "unique_sig_event = np.unique(np.asarray(sig['ntuplePupSingle']['tree']['event'].array()))\n",
    "\n",
    "#Load model and select out the tau\n",
    "model = load_model('../models/L1Tau_v3_corrected.h5')\n",
    "\n",
    "#Sig\n",
    "sig_input = sig['ntuplePupSingle']['tree']['m_inputs'].array()\n",
    "\n",
    "pt_sig = sig['ntuplePupSingle']['tree']['pt'].array()\n",
    "deltaR_sig = sig['ntuplePupSingle']['tree']['gendr1'].array()\n",
    "selection_sig = (pt_sig > 20) &(abs(deltaR_sig) < 0.4)\n",
    "\n",
    "sig_id = np.asarray(sig['ntuplePupSingle']['tree']['event'].array()[selection_sig])\n",
    "\n",
    "X_sig = np.asarray(sig_input[selection_sig])\n",
    "y_sig = model.predict(X_sig)\n",
    "\n",
    "#Bkg\n",
    "bkg_input = np.asarray(bkg['ntuplePupSingle']['tree']['m_inputs'].array())\n",
    "bkg_id = np.asarray(bkg['ntuplePupSingle']['tree']['event'].array())\n",
    "bkg_pt = np.asarray(bkg['ntuplePupSingle']['tree']['pt'].array()) \n",
    "selection_bkg = bkg_pt > 20\n",
    "bkg_id_pt = bkg_id[selection_bkg]\n",
    "\n",
    "total_n_minbias = np.unique(bkg_id).shape[0]\n",
    "total_n_sig = np.intersect1d(np.unique(bkg_id), np.unique(sig_id)).shape[0]\n",
    "\n",
    "y_bkg = model.predict(bkg_input[selection_bkg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_score_edges = [round(i,2) for i in np.arange(0, 0.9, 0.01).tolist()]+\\\n",
    "                  [round(i,4) for i in np.arange(0.9, 1, 0.0001)] + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_list = []\n",
    "bkg_list = []\n",
    "\n",
    "for tau_score_cut in tau_score_edges:\n",
    "    \n",
    "    bkg_pass = np.unique(bkg_id_pt[y_bkg.flatten() > tau_score_cut]).shape[0]\n",
    "    sig_pass = np.unique(sig_id[y_sig.flatten() > tau_score_cut])\n",
    "    \n",
    "    real_sig_pass = np.intersect1d(np.unique(bkg_id),sig_pass).shape[0]\n",
    "    \n",
    "    sig_list.append(real_sig_pass/total_n_sig)\n",
    "    bkg_list.append(bkg_pass/total_n_minbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_list_scaled = [i*(32e+3) for i in bkg_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sig_list_v3.npy', np.asarray(sig_list))\n",
    "np.save('bkg_list_v3.npy', np.asarray(bkg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sig_list, bkg_list_scaled, label='Retrained Tau NN')\n",
    "plt.ylabel(r'$\\tau_h$ Trigger Rate [kHz]')\n",
    "plt.xlabel(r'$\\tau_h$ $epsilon$ > 20 GeV')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about monotonic training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import stuff\n",
    "import uproot4\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import curve_fit?\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import optparse\n",
    "import importlib\n",
    "import pathlib\n",
    "\n",
    "import hist\n",
    "from hist import Hist\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.style.ROOT)\n",
    "\n",
    "# import matplotlib.pylab as pylab\n",
    "# params = {'legend.fontsize': 'medium',\n",
    "#          'axes.labelsize': 'x-large',\n",
    "#          'axes.titlesize':'x-large',\n",
    "#          'xtick.labelsize':'medium',\n",
    "#          'ytick.labelsize':'medium'}\n",
    "# pylab.rcParams.update(params)\n",
    "\n",
    "# #line thickness\n",
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['lines.linewidth'] = 5\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monotonenorm import direct_norm, SigmaNet, GroupSort\n",
    "\n",
    "\n",
    "monotonic = True\n",
    "LIP = 5  # lipschitz constant of the model\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(dir_path):\n",
    "    \n",
    "    #Might have to change the version for other ntuple files\n",
    "    sig = uproot4.open(dir_path+\"/test_sig_v8.root\")\n",
    "    bkg = uproot4.open(dir_path+\"/test_bkg_v8.root\")\n",
    "    \n",
    "    sig_input = sig['ntuplePupSingle']['tree']['m_inputs'].array()\n",
    "    bkg_input = bkg['ntuplePupSingle']['tree']['m_inputs'].array()\n",
    "    \n",
    "    pt_sig = sig['ntuplePupSingle']['tree']['pt'].array()\n",
    "    deltaR_sig = sig['ntuplePupSingle']['tree']['gendr1'].array()\n",
    "    selection_sig = (pt_sig > 20) & (abs(deltaR_sig) < 0.4)\n",
    "    \n",
    "    pt_bkg = bkg['ntuplePupSingle']['tree']['pt'].array()\n",
    "    deltaR_bkg = bkg['ntuplePupSingle']['tree']['gendr1'].array()\n",
    "    selection_bkg = pt_bkg > 20\n",
    "    \n",
    "    #Maybe better to use 2.4 for eta\n",
    "    \n",
    "    #Inputs: pt, eta, phi, particle id(one hot encoded)\n",
    "    X_sig = np.asarray(sig_input[selection_sig])\n",
    "    y_sig = np.full(X_sig.shape[0], 1)\n",
    "    \n",
    "    X_bkg = np.asarray(bkg_input[selection_bkg])\n",
    "    y_bkg = np.full(X_bkg.shape[0], 0)\n",
    "    \n",
    "    X = np.concatenate([X_sig, X_bkg])\n",
    "    y = np.concatenate([y_sig, y_bkg])\n",
    "    \n",
    "    index = np.arange(X.shape[0])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(X, y, index, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return torch.from_numpy(X_train), torch.from_numpy(X_test), torch.from_numpy(y_train), torch.from_numpy(y_test), torch.from_numpy(index_train), torch.from_numpy(index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, index_train, index_test = create_training_data(\"../ntuples/Jan_17_2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraint_index_part = [1] + [0]*7\n",
    "contraint_index = contraint_index_part*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(monotonic):\n",
    "    def lipschitz_norm(module):\n",
    "          return direct_norm(\n",
    "              module,  # the layer to constrain\n",
    "              \"one-inf\",  # |W|_1 constraint type\n",
    "              max_norm=LIP ** (1 / 4),  # norm of the layer (LIP ** (1/nlayers))\n",
    "              )\n",
    "\n",
    "    model = torch.nn.Sequential(\n",
    "      lipschitz_norm(torch.nn.Linear(80, 25)),\n",
    "      GroupSort(5),\n",
    "      lipschitz_norm(torch.nn.Linear(25, 10)),\n",
    "      GroupSort(5),\n",
    "      lipschitz_norm(torch.nn.Linear(10, 10)),\n",
    "      GroupSort(5),\n",
    "      lipschitz_norm(torch.nn.Linear(10, 1)),\n",
    "    )\n",
    "\n",
    "    if monotonic:\n",
    "        model = SigmaNet(\n",
    "          model,\n",
    "          sigma=LIP,\n",
    "          monotone_constraints=contraint_index,\n",
    "          # 0: don't constrain feature monotonicity,\n",
    "          # 1: monotonically increasing,\n",
    "          # -1: monotonically decreasing\n",
    "          # for each feature individually\n",
    "        )\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(monotonic)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train)\n",
    "    loss = binary_cross_entropy_with_logits(y_pred, y_train.view(-1, 1).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.sigmoid(model(X_test))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred.detach().numpy())\n",
    "\n",
    "auc_score = round(auc(fpr, tpr),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create plot for ROC\n",
    "plt.figure(1)\n",
    "\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr, tpr, label = 'Monotonic Tau NN, AUC: {}'.format(auc_score))\n",
    "\n",
    "#Establish labels and save image\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the test indices\n",
    "test_index = np.load(\"../ntuples/Jan_17_2023/index_test.npy\")\n",
    "\n",
    "sig = uproot4.open(\"../ntuples/Jan_17_2023/test_sig_v8.root\")\n",
    "tau_pt = sig['ntuplePupSingle']['tree']['pt'].array()[test_index]\n",
    "\n",
    "#Get all the inputs\n",
    "sig_input = sig['ntuplePupSingle']['tree']['m_inputs'].array()[test_index]\n",
    "\n",
    "pt_sig = sig['ntuplePupSingle']['tree']['pt'].array()[test_index]\n",
    "deltaR_sig = sig['ntuplePupSingle']['tree']['gendr1'].array()[test_index]\n",
    "selection_sig = (pt_sig > 20) &(abs(deltaR_sig) < 0.4)\n",
    "\n",
    "#Selected out the tau pt with the basic cuts first\n",
    "tau_pt_cut = tau_pt[selection_sig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sig = torch.from_numpy(np.asarray(sig_input[selection_sig]))\n",
    "y_sig = torch.sigmoid(model(X_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_cut = y_sig.detach().numpy().flatten() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_pt_nn = np.asarray(tau_pt_cut)[nn_cut]\n",
    "\n",
    "#Old nn\n",
    "old_tau_select = np.asarray(sig['ntuplePupSingle']['tree']['passLoose'].array()[test_index][selection_sig])\n",
    "tau_pt_old_nn = tau_pt_cut[old_tau_select == 1]\n",
    "\n",
    "#Fill two plots and divide the two.\n",
    "pT_egdes = [0,10,15,20,25,30,35,40,45,50,55,60,70,80,100,125,150] #200?\n",
    "pT_axis = hist.axis.Variable(pT_egdes, name = r\"$\\tau_h$ $p_T$\")\n",
    "\n",
    "hist_all_tau = Hist(pT_axis)\n",
    "hist_selected_tau = Hist(pT_axis)\n",
    "hist_selected_old_tau = Hist(pT_axis)\n",
    "\n",
    "hist_all_tau.fill(tau_pt_cut)\n",
    "hist_selected_tau.fill(tau_pt_nn)\n",
    "hist_selected_old_tau.fill(tau_pt_old_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 12))\n",
    "main_ax_artists, eff_new_nn_artists = hist_selected_tau.plot_ratio(\n",
    "    hist_all_tau,\n",
    "    rp_num_label=r\"Selected Taus (Monotonic NN Score > 0.5)\",\n",
    "    rp_denom_label=r\"All Taus [abs(gendr1) < 0.4, pt > 20GeV]\",\n",
    "    rp_uncert_draw_type=\"bar\",\n",
    "    rp_uncertainty_type=\"efficiency\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 12))\n",
    "main_ax_artists, eff_old_nn_artists = hist_selected_old_tau.plot_ratio(\n",
    "    hist_all_tau,\n",
    "    rp_num_label=r\"Selected Taus (Old NN)\",\n",
    "    rp_denom_label=r\"All Taus [abs(gendr1) < 0.4, pt > 20GeV]\",\n",
    "    rp_uncert_draw_type=\"bar\",\n",
    "    rp_uncertainty_type=\"efficiency\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The real efficiency plot\n",
    "fig = plt.figure()\n",
    "\n",
    "eff_new_nn_x = [eff_new_nn_artists.bar.patches[i].get_x() for i in range(len(eff_new_nn_artists.bar.patches))]\n",
    "eff_new_nn_y = [eff_new_nn_artists.bar.patches[i].get_y() for i in range(len(eff_new_nn_artists.bar.patches))]\n",
    "eff_new_nn_err = eff_new_nn_artists.bar.datavalues\n",
    "\n",
    "eff_old_nn_x = [eff_old_nn_artists.bar.patches[i].get_x() for i in range(len(eff_old_nn_artists.bar.patches))]\n",
    "eff_old_nn_y = [eff_old_nn_artists.bar.patches[i].get_y() for i in range(len(eff_old_nn_artists.bar.patches))]\n",
    "eff_old_nn_err = eff_old_nn_artists.bar.datavalues\n",
    "\n",
    "plt.errorbar(eff_new_nn_x, eff_new_nn_y, yerr=eff_new_nn_err, fmt='o', label = 'Monotonic Tau NN')\n",
    "plt.errorbar(eff_old_nn_x, eff_old_nn_y, yerr=eff_old_nn_err, fmt='o', label = 'Old Tau NN')\n",
    "\n",
    "plt.hlines(1, 0, 150, linestyles='dashed', color='black', linewidth=3)\n",
    "plt.ylim([0,1.1])\n",
    "plt.xlim([0,150])\n",
    "plt.xlabel(r\"$\\tau_h$ $p_T$\")\n",
    "plt.ylabel(\"Efficiency\")\n",
    "plt.legend(loc = 'center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_eta = sig['ntuplePupSingle']['tree']['eta'].array()[test_index]\n",
    "tau_eta_cut = tau_eta[selection_sig]\n",
    "tau_eta_nn = np.asarray(tau_eta_cut)[nn_cut]\n",
    "\n",
    "#Old nn\n",
    "old_tau_select = np.asarray(sig['ntuplePupSingle']['tree']['passLoose'].array()[test_index][selection_sig])\n",
    "tau_eta_old_nn = tau_eta_cut[old_tau_select == 1]\n",
    "\n",
    "#Fill two plots and divide the two.\n",
    "eta_egdes = [-2.5,-2.3,-2.0,-1.8,-1.6,-1.4,-1.0,-0.6,-0.2,0.2,0.6,1.0,1.4,1.6,1.8,2.0,2.3,2.5]\n",
    "eta_axis = hist.axis.Variable(eta_egdes, name = r\"$\\tau_h$ $\\eta$\")\n",
    "\n",
    "hist_all_tau = Hist(eta_axis)\n",
    "hist_selected_tau = Hist(eta_axis)\n",
    "hist_selected_old_tau = Hist(eta_axis)\n",
    "\n",
    "hist_all_tau.fill(tau_eta_cut)\n",
    "hist_selected_tau.fill(tau_eta_nn)\n",
    "hist_selected_old_tau.fill(tau_eta_old_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 12))\n",
    "main_ax_artists, eff_new_nn_artists = hist_selected_tau.plot_ratio(\n",
    "    hist_all_tau,\n",
    "    rp_num_label=r\"Selected Taus (Monotonic NN Score > 0.5)\",\n",
    "    rp_denom_label=r\"All Taus\",\n",
    "    rp_uncert_draw_type=\"bar\",\n",
    "    rp_uncertainty_type=\"efficiency\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 12))\n",
    "main_ax_artists, eff_old_nn_artists = hist_selected_old_tau.plot_ratio(\n",
    "    hist_all_tau,\n",
    "    rp_num_label=r\"Selected Taus (NN Score > 0.5)\",\n",
    "    rp_denom_label=r\"All Taus\",\n",
    "    rp_uncert_draw_type=\"bar\",\n",
    "    rp_uncertainty_type=\"efficiency\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The real efficiency plot\n",
    "fig = plt.figure()\n",
    "\n",
    "eff_new_nn_x = [eff_new_nn_artists.bar.patches[i].get_x() for i in range(len(eff_new_nn_artists.bar.patches))]\n",
    "eff_new_nn_y = [eff_new_nn_artists.bar.patches[i].get_y() for i in range(len(eff_new_nn_artists.bar.patches))]\n",
    "eff_new_nn_err = eff_new_nn_artists.bar.datavalues\n",
    "\n",
    "eff_old_nn_x = [eff_old_nn_artists.bar.patches[i].get_x() for i in range(len(eff_old_nn_artists.bar.patches))]\n",
    "eff_old_nn_y = [eff_old_nn_artists.bar.patches[i].get_y() for i in range(len(eff_old_nn_artists.bar.patches))]\n",
    "eff_old_nn_err = eff_old_nn_artists.bar.datavalues\n",
    "\n",
    "plt.errorbar(eff_new_nn_x, eff_new_nn_y, yerr=eff_new_nn_err, fmt='o', label = 'Retrained Tau NN')\n",
    "plt.errorbar(eff_old_nn_x, eff_old_nn_y, yerr=eff_old_nn_err, fmt='o', label = 'Old Tau NN')\n",
    "\n",
    "plt.hlines(1, -2.5, 2.5, linestyles='dashed', color='black', linewidth=3)\n",
    "plt.ylim([0,1.1])\n",
    "plt.xlim([-2.5,2.5])\n",
    "plt.xlabel(r\"$\\tau_h$ $\\eta$\")\n",
    "plt.ylabel(\"Efficiency\")\n",
    "plt.legend(loc = 'center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the rate for monotonic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = uproot4.open(\"../ntuples/Jan_17_2023/test_sig_v8.root\")\n",
    "bkg = uproot4.open(\"../ntuples/Jan_17_2023/test_bkg_v8.root\")\n",
    "\n",
    "unique_bkg_event = np.unique(np.asarray(bkg['ntuplePupSingle']['tree']['event'].array()))\n",
    "unique_sig_event = np.unique(np.asarray(sig['ntuplePupSingle']['tree']['event'].array()))\n",
    "\n",
    "#Sig\n",
    "sig_input = sig['ntuplePupSingle']['tree']['m_inputs'].array()\n",
    "\n",
    "pt_sig = sig['ntuplePupSingle']['tree']['pt'].array()\n",
    "deltaR_sig = sig['ntuplePupSingle']['tree']['gendr1'].array()\n",
    "selection_sig = (pt_sig > 20) &(abs(deltaR_sig) < 0.4)\n",
    "\n",
    "sig_id = np.asarray(sig['ntuplePupSingle']['tree']['event'].array()[selection_sig])\n",
    "\n",
    "X_sig = torch.from_numpy(np.asarray(sig_input[selection_sig]))\n",
    "y_sig = torch.sigmoid(model(X_sig))\n",
    "\n",
    "#Bkg\n",
    "bkg_input = np.asarray(bkg['ntuplePupSingle']['tree']['m_inputs'].array())\n",
    "bkg_id = np.asarray(bkg['ntuplePupSingle']['tree']['event'].array())\n",
    "bkg_pt = np.asarray(bkg['ntuplePupSingle']['tree']['pt'].array()) \n",
    "selection_bkg = bkg_pt > 20\n",
    "bkg_id_pt = bkg_id[selection_bkg]\n",
    "\n",
    "total_n_minbias = np.unique(bkg_id).shape[0]\n",
    "total_n_sig = np.intersect1d(np.unique(bkg_id), np.unique(sig_id)).shape[0]\n",
    "\n",
    "y_bkg = torch.sigmoid(model(torch.from_numpy(bkg_input[selection_bkg])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_score_edges = [round(i,2) for i in np.arange(0, 0.9, 0.01).tolist()]+\\\n",
    "                  [round(i,4) for i in np.arange(0.99, 1, 0.0001)] + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_list = []\n",
    "bkg_list = []\n",
    "\n",
    "for tau_score_cut in tau_score_edges:\n",
    "    \n",
    "    bkg_pass = np.unique(bkg_id_pt[y_bkg.flatten() > tau_score_cut]).shape[0]\n",
    "    sig_pass = np.unique(sig_id[y_sig.flatten() > tau_score_cut])\n",
    "    \n",
    "    real_sig_pass = np.intersect1d(np.unique(bkg_id),sig_pass).shape[0]\n",
    "    \n",
    "    sig_list.append(real_sig_pass/total_n_sig)\n",
    "    bkg_list.append(bkg_pass/total_n_minbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_list_scaled = [i*(32e+3) for i in bkg_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sig_list_normal = np.load('sig_list_v3.npy')\n",
    "bkg_list_normal = np.load('bkg_list_v3.npy')\n",
    "bkg_list_normal_scaled = [i*(32e+3) for i in bkg_list_normal]\n",
    "\n",
    "plt.plot(sig_list_normal, bkg_list_normal_scaled, label='Normal Retrained Tau NN',linewidth=5)\n",
    "plt.plot(sig_list, bkg_list_scaled, label='Monotonic Tau NN',linewidth=5)\n",
    "plt.ylabel(r'$\\tau_h$ Trigger Rate [kHz]')\n",
    "plt.xlabel(r'$\\tau_h$ $epsilon$ > 20 GeV')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
